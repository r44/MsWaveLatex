\chapter{Introduction}
\label{c:intro}

With the arrival of big-data era, similarity search for various types of data among distributed machines such as  mobile devices has become a common and important task.  In a distributed environment where a large amount of local machines are involved in computation and storage, our goal is to minimize the amount of transmission cost while identifying the nearest neighbors of a query. This paper aims to generalize the current state-of-the-art on distributed pattern matching from only suitable for ``time series''  datasets to other types of data such as image, audio, etc.

% Scenario

Consider a scenario in which, we are building a search engine on a large amount of distributed machines such as smart phones or tabelets.  On that engine, users could search for the most similar instances from other devices as long as they also share their data.  These types of these instances could be image, audio, which are very common in these devices. As a search engine, it would obtain a large number of users whose number of queries could be also very large.  So it is not pratical to directly send every query to the all devices.  Also, since there could be a very large amount of instances on each machine, it is also inefficient to send them back to the search engine due to its huge bandwidth cost.

% Challenge
The proposed task is challenging in several aspects. First, the goal is to identify the \emph{exact} $k$ nearest neighbors with guarantee, instead of approximated $k$ nearest neighbors.  Second, we are targeting at handling endless number of queries (simliar to a search engine) and assume the data are stored distributed in many (thousands or even millions) local devices.  Finally, different from most of the previous works to focus only on time-series data, we are not constraining ourselves to a specific type of data, rather emphasizing on a general-purpose algorithm.

% Weakness of LeeWave.
One of the state-of-the-art method, LeeWave,\cite{LeeWave} has been proposed to handle some of the above challenges.  Unfortunately, it is designed specific for time-series and works poorly for other types of data.  The main reason is that the method LeeWave used to prune the candidates assumes the smoothness of data, which is not necessary true in general case.  Another problem is that the cost of the pruning procedure in LeeWave grows linearly with the number of total instances in these distrubited devices.  When the number of instances becomes large, so is the communication cost.

% Our model.

We propose a two-phase framework to solve these challenges.  On the basis of LeeWave, our proposed model also derives theoretical bounds for pruning candidates in the early stages.  To further improve the performance of pruning, we add a preprocessing step which utilizes the idea of orthogonal transformation to directly optimizes these bounds given data.  We also devise a method which reduces the cost during the pruning procedure from linear to the number of total instances to a constant value, which could significantly reduce the communication cost given large-scale data.


% Contribution
We summarize our main contributions as follows:
\begin{enumerate}
	\item We propose a general communication-efficient framework to identify exact $k$NN instances given a query. The model can be applied to various types of datasets which are distributed in a large amount of devices.
	\item In the part of methodology, based on the ideas of upper bounds and lower bounds of similarity in \cite{LeeWave}, we derive new bounds with the help of the orthogonal transformation which enhance the power of pruning to save communication cost.  Besides, we propose a method which reduces the cost for the pruning procedure from linear to the number of total instances to a constant value.  Moreover, we give a method to estimate the communication cost of a query before sending it, which allows us to dynamically adjust how much information we need to send for it based on the history of past queries.
	\item We conduct extensive experiments for various types of datasets to demonstrate the effectiveness of our model.
\end{enumerate}

This thesis is organized as follows: In the chapter 2, we discuss the related works and their limitations.  In the chapter 3, we describe the details of our framework. In the chapter 4, we provide the results of the experiments using images, audio, and time series datasets, and conclude our works in chapter 5.

%\bibliographystyle{unsrt}
%\bibliography{thesisbib}